{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8] [ 9 10 11 12 13 14 15 16 17]\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "Loading time 0.01972031593322754\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "1887\n",
      "12750\n",
      "[ 9 10 11]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "(1887, 64) (1887,)\n",
      "num_rounds =  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul/anaconda3/envs/Keras/lib/python3.6/site-packages/ipykernel_launcher.py:245: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 88 clusters\n",
      "Partition 1: 20 clusters\n",
      "Partition 2: 5 clusters\n",
      "Label not found, adding a new class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [00:00<00:00, 97541.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label not found, adding a new class\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import MultipleEVM\n",
    "import EVM\n",
    "import h5py\n",
    "import torch\n",
    "import time\n",
    "from IPython.utils import io\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "#import seaborn as sn\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "try:\n",
    "    import cPickle\n",
    "except:\n",
    "    import _pickle as cPickle\n",
    "    \n",
    "    \n",
    "import os\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from finch import FINCH\n",
    "import cv2\n",
    "from B3 import calc_b3\n",
    "\n",
    "from MultipleEVM import MultipleEVM\n",
    "from torch.cuda.amp import autocast \n",
    "\n",
    "#import sys\n",
    "#orig_stdout = sys.stdout\n",
    "#f = open('opensense_owm.txt', 'w')\n",
    "#sys.stdout = f\n",
    "\n",
    "distance_function='cosine'\n",
    "device = torch.device(\"cuda\") #CHANGE TO \"cpu\" for RPI\n",
    "delta = 0.35\n",
    "tailsize=100\n",
    "cover_threshold=0.7\n",
    "distance_multiplier=0.4\n",
    "nb_classes = 18\n",
    "sudo_label = np.arange(nb_classes)\n",
    "init_nb_cl = 9\n",
    "nb_cl = 3\n",
    "nb_groups = 4\n",
    "\n",
    "#LOAD EVM features\n",
    "EVM_features_datasets = str(nb_cl)+'EVM_features_PAMAP2_init.pickle'\n",
    "with open(EVM_features_datasets,'rb') as fp:\n",
    "    X_sep_features = cPickle.load(fp)\n",
    "    X_sep_label = cPickle.load(fp)\n",
    "    X_sep_features_val = cPickle.load(fp)\n",
    "    X_sep_label_val = cPickle.load(fp)\n",
    "    X_sep_features_test = cPickle.load(fp)\n",
    "    X_sep_label_test = cPickle.load(fp)\n",
    "    \n",
    "with open(str(nb_cl)+'PAMAP2_mixing_init.pickle','rb') as fp:\n",
    "    mixing = cPickle.load(fp)\n",
    "    \n",
    "known_list = mixing[0:9]\n",
    "unknown_list = mixing[9:18]\n",
    "print(known_list,unknown_list)\n",
    "groups = [[9, 10, 11],[12, 13, 14],[15, 16, 17]]\n",
    "test_featrues = []\n",
    "test_labels = []\n",
    "for i in known_list:\n",
    "    test_featrues.extend(X_sep_features_test[i])\n",
    "    test_labels.extend(X_sep_label_test[i])\n",
    "\n",
    "print( np.unique(test_labels))\n",
    "for iteration in range(3):\n",
    "    owl_train_labels = []\n",
    "    owl_train_featrues = []\n",
    "\n",
    "    for i in (groups[iteration]):\n",
    "        owl_train_featrues.extend(X_sep_features[i])\n",
    "        owl_train_labels.extend(X_sep_label[i])\n",
    "        owl_train_featrues.extend(X_sep_features_val[i])\n",
    "        owl_train_labels.extend(X_sep_label_val[i])\n",
    "        test_featrues.extend(X_sep_features_test[i])\n",
    "        test_labels.extend(X_sep_label_test[i])\n",
    "    #print(len(owl_train_featrues))\n",
    "    #print(len(test_featrues))\n",
    "    #print( np.unique(owl_train_labels))\n",
    "    #print( np.unique(test_labels))\n",
    "    \n",
    "def compute_B3(y,preds_cat):\n",
    "    L = y#true_labels\n",
    "    C = preds_cat#pred_class_prob\n",
    "\n",
    "    is_known = (L>=0) * (L<9)\n",
    "    is_unknown = ~is_known\n",
    "    predicted_known = (C>=0) * (C<9)\n",
    "    predicted_unknown = ~predicted_known\n",
    "\n",
    "    #print(is_known,is_unknown)\n",
    "\n",
    "    N_KK =  np.sum(is_known*predicted_known)\n",
    "    N_KU =  np.sum(is_known*predicted_unknown)\n",
    "    N_UK =  np.sum(is_unknown*predicted_known)\n",
    "    N_UU =  np.sum(is_unknown*predicted_unknown)\n",
    "\n",
    "    N_ALL = N_KK + N_KU + N_UK + N_UU\n",
    "\n",
    "\n",
    "    LKK = L[is_known*predicted_known]\n",
    "    CKK = C[is_known*predicted_known]\n",
    "    LUU = L[is_unknown*predicted_unknown]\n",
    "    CUU = C[is_unknown*predicted_unknown]\n",
    "\n",
    "    if N_KK > 0:\n",
    "      correct = np.sum(LKK==CKK)\n",
    "    else:\n",
    "      correct = 0\n",
    "    if N_UU > 0:\n",
    "      b3, _, _ = calc_b3(L = LUU , K = CUU)\n",
    "    else:\n",
    "      b3 = 0 \n",
    "    OWM = ( correct +  ( b3 * N_UU ) ) /  N_ALL\n",
    "\n",
    "    print(\"OWM = \",OWM)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class OpenSense(object):\n",
    "  def __init__(self,\n",
    "         csv_folder, cores, detection_threshold):    \n",
    "\n",
    "\n",
    "    self.csv_folder = csv_folder\n",
    "    self.cores = cores\n",
    "    self.detection_threshold = detection_threshold\n",
    "\n",
    "    self.T = detection_threshold\n",
    "    self.UU = 0\n",
    "    \n",
    "    self.queue_dict = {} #empty dictionary\n",
    "    self.clustered_set= set() #empty set\n",
    "    self.clustered_dict= {}\n",
    "    \n",
    "\n",
    "    self.rho = number_of_unknown_to_create_evm\n",
    "    self.psi = number_of_unknown_to_strat_clustering\n",
    "    self.number_known_classes = N_known_classes \n",
    "    \n",
    "\n",
    "    # initialize EVM\n",
    "\n",
    "\n",
    "    self.evm = MultipleEVM(tailsize=tailsize,\n",
    "                 cover_threshold=cover_threshold,\n",
    "                 distance_multiplier=distance_multiplier)\n",
    "    self.evm.load(evm_model_path)\n",
    "    \n",
    "\n",
    "  def test_B3(self, features, labels):\n",
    "    FVs = torch.from_numpy(features)\n",
    "    Pr = self.evm.class_probabilities(FVs)\n",
    "    pred_class_prob = np.argmax(Pr,axis=1) #+ 1\n",
    "    true_labels = np.asarray(labels)#.argmax(axis=1)\n",
    "    #accuracy = accuracy_score(true_labels, pred_class_prob)\n",
    "    #macro = f1_score(true_labels, pred_class_prob, average='macro')\n",
    "    #print('testaccuracy', accuracy)\n",
    "    #print('tetstmacro', macro)\n",
    "    #cm = confusion_matrix(true_labels, pred_class_prob)\n",
    "    #print(cm)\n",
    "    #size = max(len(np.unique(true_labels)),len(np.unique(pred_class_prob)))\n",
    "    #print(size)\n",
    "    #df_cm = pd.DataFrame(cm, range(size), range(size))\n",
    "    #normed_c = (df_cm.T / df_cm.astype(np.float).sum(axis=1)).T\n",
    "    #plt.figure(figsize=(16,12))\n",
    "    #sn.set(font_scale=1.4) # for label size\n",
    "    #sn.heatmap(normed_c, annot=True, annot_kws={\"size\": 16},fmt='.2%', cmap='Blues') # font size\n",
    "    #plt.show()\n",
    "    compute_B3(true_labels,pred_class_prob)\n",
    "    \n",
    "  def feature_extraction(self, test_features, test_labels):\n",
    "    #load features\n",
    "    len_ = test_features.shape[0]\n",
    "    self.features_dict = {}\n",
    "    self.label_dict = {}\n",
    "\n",
    "    for index, feature, label in zip(range(len_),test_features,test_labels):\n",
    "        self.features_dict[index] = test_features[index]\n",
    "        self.label_dict[index] = test_labels[index]\n",
    "    \n",
    "    return self.features_dict\n",
    "    \n",
    "  def ow_classification(self, round_id):\n",
    "\n",
    "    \n",
    "    \n",
    "    result_path = os.path.join(self.csv_folder, \n",
    "                  f\"class_\" + str(round_id).zfill(2)+\".csv\")\n",
    "    image_names, FVs = zip(*self.features_dict.items()) \n",
    "    FVs = np.asarray(FVs, dtype=np.float32)\n",
    "    FVs = torch.from_numpy(FVs)\n",
    "\n",
    "    Pr = self.evm.class_probabilities(FVs)\n",
    "    Pr = torch.tensor(Pr)\n",
    "    Pm,_ = torch.max(Pr, dim=1)\n",
    "    pu = 1 - Pm\n",
    "    #print('SHAPES',pu.shape,Pr.shape)\n",
    "    all_rows_tensor = torch.cat((pu.view(-1,1), Pr), 1)\n",
    "    #print('SHAPES',all_rows_tensor.shape)\n",
    "\n",
    "    norm = torch.norm(all_rows_tensor, p=1, dim=1)\n",
    "    normalized_tensor = all_rows_tensor/norm[:,None]\n",
    "    col1 = ['id', 'P_unknown']\n",
    "    col2 = ['P_'+str(k) for k in range(1, self.number_known_classes+1)]\n",
    "    col3 = ['U_'+str(k) for k in range(1, self.UU+1)]\n",
    "    col = col1 + col2 + col3\n",
    "\n",
    "\n",
    "    self.df_classification = pd.DataFrame(zip(image_names,*normalized_tensor.t().tolist()), columns=col)\n",
    "    self.df_classification.to_csv(result_path, index = False, header = False, float_format='%.4f')\n",
    "    \n",
    "    result_path_raw = os.path.join(self.csv_folder, \n",
    "                  f\"raw_class_\" + str(round_id).zfill(2)+\".csv\")\n",
    "    self.df_class_raw = pd.DataFrame(zip(image_names,*all_rows_tensor.t().tolist()), columns=col)\n",
    "    self.df_class_raw.to_csv(result_path_raw, index = False, header = False, float_format='%.4f')\n",
    "    return result_path,self.df_classification\n",
    "\n",
    "\n",
    "  def model_updating(self, features, df_classification, round_id=None):\n",
    "    \"\"\"\n",
    "    Update evm models\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    m = -2\n",
    "    nu = 0 \n",
    "    for k, row in df_classification.iterrows():\n",
    "        if row[1] > self.T:  # predicted unknown unknown #before > greater\n",
    "            self.queue_dict[k] = features[k] \n",
    "        if len(self.queue_dict) >= self.psi:\n",
    "            data =  np.vstack(self.queue_dict.values())\n",
    "            c_all, num_clust, req_c = FINCH(data, verbose=True)\n",
    "            cluster_labels = c_all[:,-1]\n",
    "            m = num_clust[-1]  # number of clusters after clustering. \n",
    "            to_be_delete = []\n",
    "        if m >= 2:\n",
    "            FVsn_queue, FVs_queue = zip(*self.queue_dict.items())\n",
    "            if len(self.clustered_dict)>0:\n",
    "                image_names_clustered, FVs_clustered = zip(*self.clustered_dict.items())\n",
    "            else:\n",
    "                FVs_clustered=[]\n",
    "                for k in range(m):  # number of clusters after clustering. \n",
    "                    index = [i for i in range(len(cluster_labels)) if cluster_labels[i] == k]\n",
    "                    index_neg = [i for i in range(len(cluster_labels)) if cluster_labels[i] != k]\n",
    "                    if len(index) > self.rho:\n",
    "                        to_be_delete = to_be_delete + index\n",
    "                        nu = nu+1\n",
    "                        FV_positive = torch.from_numpy(np.array([FVs_queue[k] for k in index]))\n",
    "                        FV_negative_1 = [FVs_queue[k] for k in index_neg]\n",
    "                        FV_negative_2 = list(FVs_clustered)\n",
    "                        FV_negative = torch.from_numpy(np.array(FV_negative_1 + FV_negative_2))\n",
    "                        y = self.number_known_classes + self.UU + nu\n",
    "                        # Train a new EVM with FV_positive and [FV_negative_1+FVs_clustered] as negatives\n",
    "                        # Insert the new EVM to new_EVM_list\n",
    "                        self.evm.train_update(new_points = FV_positive, label = (y-1), distance_multiplier = unknown_dm , extra_negatives = FV_negative )\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    if nu > 0:\n",
    "        fv_covered = []\n",
    "        for k in (to_be_delete):\n",
    "            fv_covered.append(FVsn_queue[k])\n",
    "        for name in fv_covered:\n",
    "            fv_name = self.queue_dict[name]\n",
    "            self.clustered_dict.update({name:fv_name})\n",
    "        del self.queue_dict[name]\n",
    "    self.UU = self.UU + nu\n",
    "    #print(\"End: len(self.clustered_dict) = \", len(self.clustered_dict))\n",
    "    #print(\"End: len(self.queue_dict) = \", len(self.queue_dict))\n",
    "    #print(f\"{nu} new evm classes added. Total discovered classes = {self.UU}\")\n",
    "    return self.evm\n",
    "\n",
    "\n",
    "\n",
    "  def save(self, name):\n",
    "    self.evm.save(f'/scratch/OpenSense_EVM_{name}.hdf5')\n",
    "\n",
    "####################################\n",
    "\n",
    "\n",
    "csv_folder = './csv_folder/PAMAP2_9/'\n",
    "if not os.path.exists(csv_folder):\n",
    "    os.makedirs(csv_folder)\n",
    "\n",
    "number_of_tests = 0\n",
    "\n",
    "N_CPU = 32 #CHANGE to 4 for RPI\n",
    "batch_size = 100 #set to 10 for RPI\n",
    "start_learning = 500\n",
    "\n",
    "evm_model_path = './data/EVM_PAMAP2_9.hdf5'\n",
    "\n",
    "feature_size = 64\n",
    "\n",
    "\n",
    "\n",
    "tailsize = 2000\n",
    "cover_threshold = 0.7\n",
    "distance_multiplier = 0.4\n",
    "unknown_dm = 0.6 #distance multiplier for unknown classes\n",
    "\n",
    "N_known_classes = 9\n",
    "number_of_unknown_to_create_evm = 70\n",
    "number_of_unknown_to_strat_clustering = 400\n",
    "\n",
    "cores = 32\n",
    "detection_threshold = 0.01#0.001 #delta\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_folder_i = csv_folder\n",
    "OpenSense_alg = OpenSense(csv_folder_i, cores, detection_threshold)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"Loading time {t1-t0}\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "groups = [[9, 10, 11],[12, 13, 14],[15, 16, 17]]\n",
    "test_featrues = []\n",
    "test_labels = []\n",
    "for i in known_list:\n",
    "\n",
    "    test_featrues.extend(X_sep_features_test[i])\n",
    "    test_labels.extend(X_sep_label_test[i])\n",
    "\n",
    "print( np.unique(test_labels))\n",
    "for iteration in range(3):\n",
    "    owl_train_labels = []\n",
    "    owl_train_featrues = []\n",
    "\n",
    "    for i in (groups[iteration]):\n",
    "        owl_train_featrues.extend(X_sep_features[i])\n",
    "        owl_train_labels.extend(X_sep_label[i])\n",
    "        owl_train_featrues.extend(X_sep_features_val[i])\n",
    "        owl_train_labels.extend(X_sep_label_val[i])\n",
    "        test_featrues.extend(X_sep_features_test[i])\n",
    "        test_labels.extend(X_sep_label_test[i])\n",
    "    print(len(owl_train_featrues))\n",
    "    print(len(test_featrues))\n",
    "    print( np.unique(owl_train_labels))\n",
    "    print( np.unique(test_labels))\n",
    "    LDSF_test_all = torch.from_numpy(np.asarray(test_featrues))\n",
    "    owl_test_data = np.asarray(test_featrues)\n",
    "    owl_test_labels = np.asarray(test_labels)\n",
    "    ds_test_features = np.asarray(owl_train_featrues)\n",
    "    ds_test_labels = np.asarray(owl_train_labels)\n",
    "\n",
    "    print(ds_test_features.shape,ds_test_labels.shape)\n",
    "    features_list = ds_test_features\n",
    "    labels_list = ds_test_labels\n",
    "    num_rounds = (len(features_list)) //batch_size\n",
    "\n",
    "    if ( (len(features_list)) % batch_size) !=0 :\n",
    "        num_rounds += 1\n",
    "\n",
    "    print(\"num_rounds = \", num_rounds) ## num_rounds in this case, basically number of batches\n",
    "    df_classification = pd.DataFrame()\n",
    "    features = {}\n",
    "    o_len = 0\n",
    "    for round_id in range(num_rounds):\n",
    "        t2 = time.time()\n",
    "\n",
    "        feature_batch = features_list[round_id*batch_size : (round_id+1)*batch_size]\n",
    "        label_batch = labels_list[round_id*batch_size : (round_id+1)*batch_size]\n",
    "\n",
    "        t3 = time.time()\n",
    "        F = OpenSense_alg.feature_extraction(feature_batch, label_batch)\n",
    "        t4 = time.time()\n",
    "        _, C = OpenSense_alg.ow_classification(round_id)\n",
    "        t5 = time.time()\n",
    "\n",
    "        c_len = len(F)\n",
    "        idx = np.arange(0,c_len)\n",
    "\n",
    "        for index in idx:\n",
    "            features[index+o_len] = F[index]\n",
    "        o_len =o_len + c_len\n",
    "        df_classification = pd.concat([df_classification, C])\n",
    "        if o_len+1 >= start_learning:\n",
    "            df_classification = df_classification.reset_index(drop=True)\n",
    "            #print(\"f len,c len,olen,clen : \", len(features),len(df_classification),o_len,c_len)\n",
    "            evm = OpenSense_alg.model_updating(features,df_classification, round_id)\n",
    "            t6 = time.time()\n",
    "            #print(\"model_updating time = \", t6-t5)\n",
    "            df_classification = pd.DataFrame()\n",
    "            features = {}\n",
    "            o_len = 0\n",
    "        #os.remove(round_file_name)\n",
    "    if o_len > 0:\n",
    "        evm = OpenSense_alg.model_updating(features,df_classification, round_id)\n",
    "        t6 = time.time()\n",
    "        #print(\"model_updating time = \", t6-t5)\n",
    "    t7 = time.time()\n",
    "\n",
    "    #print(\"round time = \", t7-t2)\n",
    "    OpenSense_alg.test_B3(owl_test_data,owl_test_labels)\n",
    "\n",
    "del OpenSense_alg\n",
    "end_time = time.time()\n",
    "#print(f\"Loading time {t1-t0}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#sys.stdout = orig_stdout\n",
    "#f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b49e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
